# Model Context Protocol - MCP

Слово авторам:
  * MAR 10, 2025 [Why MCP Won](https://www.latent.space/p/why-mcp-won) - Learnings from Anthropic's extraordinarily successful Launch and Workshop
  * APR 03, 2025 [Latent Space: The AI Engineer Podcast](https://www.latent.space/p/mcp) - When we first wrote Why MCP Won, we had no idea how quickly it was about to win. In the past 4 weeks, OpenAI and now Google have now announced the MCP support, effectively confirming our prediction that MCP was the presumptive winner of the agent standard wars. MCP has now overtaken OpenAPI.


Model Context Protocol (MCP) — это новый подход к взаимодействию с большими языковыми моделями (LLM), позволяющий им работать с внешними данными, помнить больше информации и быть более “контекстно осведомлёнными”. Если совсем просто, MCP — это способ расширить память и контекст LLM, подключив её к внешним источникам данных, которые можно динамически подгружать и управлять ими.

## Зачем нужен MCP?

Большие языковые модели, такие как GPT, Claude или Gemini, работают внутри ограниченного контекстного окна - например, могут “удерживать в голове” ограниченное количество токенов, что мало для серьезных бизнес-задач. Ещё хуже - модель ничего не помнит между сессиями: каждый раз вы начинаете с чистого листа.

Как в этом помогает MCP:
  * позволяет хранить “память” модели вне самой модели - в виде баз данных, файлов или специальных векторов;
  * даёт доступ к внешним источникам: базам знаний, API, корпоративным данным;
  * предоставляет гибкую систему управления контекстом - что, когда и зачем “показать” модели.

## В чём отличие MCP от старых подходов?

**Контекст — это не просто текст, а среда выполнения**

Раньше, когда мы хотели добавить знание в GPT (например, в ChatGPT через плагины), мы просто:
  * делали плагин;
  * по запросу дергали API или базу;
  * вставляли результат в prompt.

Это **line-by-line механика** - модель ничего не знает о данных заранее, контекст передаётся целиком каждый раз. 

MCP вместо этого **создаёт абстрактный слой**, где знания и доступ к ним организованы в формате “регистров” и “каналов”, как в реальной ОС. Он сам управляет:
  * что нужно загрузить;
  * когда это сделать;
  * как представить модели.

**Главное отличие: управление контекстом становится программируемым, модульным и переносимым.**

Представим, что вы строите AI, который помогает CEO компании, и этот ИИ должен учитывать:
  * расписание CEO,
  * документы, над которыми он работает,
  * недавние email’ы и задачи.

Если вы реализуете это через плагины — вы просто делаете десяток ручных запросов и склеиваете результат.

С MCP — вы создаёте системный уровень, где:
  * каждая сущность (календарь, документы, задачи) — это регистровый модуль, с интерфейсом load, query, hydrate;
  * модель не знает всего сразу — MCP сам решает, что ей “вспомнить” в нужный момент;
  * вы можете “приостановить”, “перезапустить” или “перенести” эту память как состояние AI.

## MCP — попытка создать стандартизированный, масштабируемый и управляемый слой памяти и знаний для LLM

Он предлагает:
  * Регистры - концепции, которые хранят определённый контекст (например, “рабочий день пользователя”);
  * Протоколы загрузки/выгрузки - что подгружать в текущий prompt;
  * Слои интерпретации - MCP может сам адаптировать данные под формат, удобный модели (chunking, summarization, embedding и т.д.);
  * Модульность и переносимость - как у Docker или Linux, вы можете “подключить” MCP к другой модели.